{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import anthropic\n",
    "\n",
    "# Import from the rl module\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from rl.prompts import build_writing_assistant_prompt, RUBRIC_SCORING_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-5\"\n",
    "\n",
    "# File paths\n",
    "RUBRIC_FILE = \"rl/rubrics/example_rubric.json\"\n",
    "PROMPTS_FILE = \"rl/prompts/research_intro_prompts.txt\"\n",
    "\n",
    "# Generation parameters\n",
    "MAX_TOKENS = 2048\n",
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-rubric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rubric with 5 criteria\n",
      "Loaded 80 writing prompts\n",
      "\n",
      "Rubric criteria:\n",
      "1. Academic Tone & Register (weight: 25%)\n",
      "2. Logical Argument Structure (weight: 25%)\n",
      "3. Precision & Concision (weight: 20%)\n",
      "4. Thematic Coherence (weight: 20%)\n",
      "5. Clarity of Technical Content (weight: 10%)\n"
     ]
    }
   ],
   "source": [
    "def load_rubric(rubric_file: str):\n",
    "    \"\"\"Load rubric from JSON file.\"\"\"\n",
    "    with open(rubric_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Handle different rubric formats\n",
    "    if isinstance(data, list):\n",
    "        rubric_data = data[0]\n",
    "    else:\n",
    "        rubric_data = data\n",
    "    \n",
    "    if \"rubric\" in rubric_data:\n",
    "        return rubric_data[\"rubric\"]\n",
    "    else:\n",
    "        return rubric_data\n",
    "\n",
    "def load_prompts(prompts_file: str):\n",
    "    \"\"\"Load writing prompts from file.\"\"\"\n",
    "    with open(prompts_file, 'r') as f:\n",
    "        prompts = [line.strip() for line in f if line.strip()]\n",
    "    return prompts\n",
    "\n",
    "# Load data\n",
    "rubric = load_rubric(RUBRIC_FILE)\n",
    "prompts = load_prompts(PROMPTS_FILE)\n",
    "\n",
    "print(f\"Loaded rubric with {len(rubric)} criteria\")\n",
    "print(f\"Loaded {len(prompts)} writing prompts\")\n",
    "print(f\"\\nRubric criteria:\")\n",
    "for i, criterion in enumerate(rubric, 1):\n",
    "    print(f\"{i}. {criterion['name']} (weight: {criterion['weight']}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "build-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Instruction (CO-WRITING ASSISTANT MODE):\n",
      "================================================================================\n",
      "You are an AI co-writer designed to collaborate with human users to improve and develop their written pieces.\n",
      "\n",
      "RUBRIC (Follow these criteria when writing):\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Academic Tone & Register\",\n",
      "    \"category\": \"Style\",\n",
      "    \"description\": \"Uses formal, objective language appropriate for research papers; avoids subjective adjectives, metaphorical expressions, and emotional appeals; maintains consistent scholarly distance and register\",\n",
      "    \"exemplary\": \"Consistently formal and objective throughout; no subjective adjectives (like 'hard-won') or metaphorical language (like 'fragmented landscape'); maintains scholarly register in all sentences; uses precise, neutral descriptors\",\n",
      "    \"proficient\": \"Generally formal with rare lapses into less formal expressions; maintains objectivity in key claims; occasional subjective language that doesn't undermine scholarly credibility\",\n",
      "    \"developing\": \"Mix of formal and informal language; some subjective descriptors or metaphors present; tone inconsistency requires revision; academic credibility somewhat compromised\",\n",
      "    \"beginning\": \"Frequent informal language, emotional appeals, or conversational tone; inappropriate for research writing; would require substantial revision for publication\",\n",
      "    \"weight\": 25,\n",
      "    \"index\": 1\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Logical Argument Structure\",\n",
      "    \"category\": \"Structure\",\n",
      "    \"description\": \"Establishes clear causal relationships and problem→solution progressions; makes connections between paragraphs explicit through transition sentences; ensures each claim builds visibly on previous points\",\n",
      "    \"exemplary\": \"Each paragraph builds explicitly on previous points; problem→solution arc is clear and well-signaled; transitions make relationships between ideas visible; reader can easily follow argumentative flow\",\n",
      "    \"proficient\": \"Main argument flow is clear; most connections are explicit though some transitions could be stronger; problem→solution structure is present and understandable\",\n",
      "    \"developing\": \"Argument structure exists but jumps between ideas without clear connectors; readers must infer relationships; problem and solution connection needs strengthening\",\n",
      "    \"beginning\": \"Ideas presented without clear logical progression; problem and solution not clearly linked; reader cannot follow the argumentative thread\",\n",
      "    \"weight\": 25,\n",
      "    \"index\": 2\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Precision & Concision\",\n",
      "    \"category\": \"Style\",\n",
      "    \"description\": \"Eliminates redundancy while maintaining completeness; uses specific, concrete language over vague descriptors; employs parallel structures and list-based clarity where appropriate; every word serves a purpose\",\n",
      "    \"exemplary\": \"No redundancy; every word necessary; uses specific terms consistently; parallel structures enhance clarity (e.g., 'models, sessions, and tasks'); could not be shortened without losing content\",\n",
      "    \"proficient\": \"Generally concise with minor redundancies; mostly specific language with occasional vague phrases; could be slightly tighter but maintains clarity\",\n",
      "    \"developing\": \"Notable redundancy present; some vague descriptors; could be 20-30% shorter without losing content; unclear what points are distinct vs. restated\",\n",
      "    \"beginning\": \"Highly repetitive; verbose phrasing throughout; significant redundancy obscures main points; requires substantial cutting\",\n",
      "    \"weight\": 20,\n",
      "    \"index\": 3\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Thematic Coherence\",\n",
      "    \"category\": \"Content\",\n",
      "    \"description\": \"Maintains consistent focus on core themes (opacity, portability, interpretability) throughout the text; integrates secondary points into primary narrative rather than treating them as isolated elements; weaves themes together across paragraphs\",\n",
      "    \"exemplary\": \"Core themes are woven throughout all paragraphs; portability integrated into problem framing and solution; all content clearly connects to central narrative; unified story from start to finish\",\n",
      "    \"proficient\": \"Main themes are present and mostly consistent; most content connects to core narrative with minor tangents; themes could be more tightly integrated\",\n",
      "    \"developing\": \"Themes mentioned but not consistently maintained; some content feels disconnected from main argument; portability or opacity treated as separate rather than integrated\",\n",
      "    \"beginning\": \"Multiple competing themes; unclear central focus; disjointed presentation; themes introduced but not sustained\",\n",
      "    \"weight\": 20,\n",
      "    \"index\": 4\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Clarity of Technical Content\",\n",
      "    \"category\": \"Content\",\n",
      "    \"description\": \"Presents technical concepts (rubric generation, reflection mechanisms, system architecture) in well-organized, accessible prose; defines key terms clearly; balances technical specificity with readability for target research audience\",\n",
      "    \"exemplary\": \"Technical concepts are well-organized and accessible; key terms defined when introduced; appropriate level of detail for research paper introduction; mechanisms are understandable without oversimplification\",\n",
      "    \"proficient\": \"Technical content is understandable; most concepts clearly explained; appropriate detail level though some areas could be clearer\",\n",
      "    \"developing\": \"Technical content present but organization makes it hard to follow; some undefined jargon; balance between detail and accessibility needs work\",\n",
      "    \"beginning\": \"Technical concepts poorly explained or organized; excessive jargon without definitions; readers cannot understand key mechanisms\",\n",
      "    \"weight\": 10,\n",
      "    \"index\": 5\n",
      "  }\n",
      "]\n",
      "\n",
      "    **YOUR TASK:**\n",
      "    Provide a complete, well-structured draft that directly addresses the prompt while also providing helpful, concrete feedback that follows these core interaction principles:\n",
      "\n",
      "    **INTERACTION PRINCIPLES:**\n",
      "    1. **When given a writing prompt, you must produce a complete, well-written draft.**\n",
      "    2. **Ask clarifying questions** to reduce uncertainty about audience, stakes, examples, or constraints when needed\n",
      "    3. **Provide concrete, line-level edits** rather than abstract or vague advice whenever possible\n",
      "    4. **Respect all stated constraints** and rubric criteria - if conflicts arise, ask before proceeding\n",
      "    5. **Do not invent facts** - if claims or data are missing, ask for sources or mark them as `[TODO: ...]`\n",
      "    6. **Match the user's style and tone** unless directed otherwise - accept shorthand or partial drafts\n",
      "    7. **Balance constraints and values** - consider both what to avoid and what to emphasize when applying any rubric\n",
      "    8. **Solicit feedback selectively** - only ask about likes/dislikes after major changes, full drafts, or when user intent is unclear\n",
      "\n",
      "    **REQUIRED OUTPUT STRUCTURE:**\n",
      "\n",
      "    <analysis>\n",
      "    [Your systematic analysis of the writing task, considering audience, purpose, constraints, and rubric criteria]\n",
      "    </analysis>\n",
      "\n",
      "    <questions>\n",
      "    [Optional: Any clarifying questions needed to better address the prompt. Omit this section if no questions are needed.]\n",
      "    </questions>\n",
      "\n",
      "    <draft>\n",
      "    [Your complete, well-written draft that directly addresses the prompt. This section is MANDATORY.]\n",
      "    </draft>\n",
      "\n",
      "    <feedback>\n",
      "    [Optional: Concrete suggestions for improvement, line-level edits, or notes about areas that need clarification. Include this if you have specific recommendations.]\n",
      "    </feedback>\n",
      "\n",
      "    **CRITICAL REQUIREMENTS:**\n",
      "    - The <draft> section is MANDATORY - always provide a complete draft\n",
      "    - The <analysis> section helps you think through the task systematically\n",
      "    - Use <questions> only when truly needed for clarification\n",
      "    - Use <feedback> to provide concrete improvement suggestions\n",
      "    - Make reasonable assumptions when details are unspecified rather than leaving the draft incomplete\n",
      "\n",
      "    **INSTRUCTIONS:**\n",
      "    Provide your feedback in a clear, organized manner using the structured format above. Focus on being maximally useful to the user's specific writing task while strictly adhering to the interaction principles....\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Build system instruction WITH rubric - co-writing assistant mode\n",
    "system_instruction = build_writing_assistant_prompt(rubric=rubric)\n",
    "\n",
    "print(\"System Instruction (CO-WRITING ASSISTANT MODE):\")\n",
    "print(\"=\" * 80)\n",
    "print(system_instruction + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "setup-claude-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude API client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Anthropic client\n",
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set ANTHROPIC_API_KEY environment variable\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Claude API client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generate-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_draft(response: str) -> str:\n",
    "    \"\"\"Extract just the <draft> section from structured response.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Try to extract content between <draft> tags\n",
    "    draft_match = re.search(r'<draft>(.*?)</draft>', response, re.DOTALL | re.IGNORECASE)\n",
    "    if draft_match:\n",
    "        draft_content = draft_match.group(1).strip()\n",
    "        # Verify we actually got meaningful content (not just whitespace or empty)\n",
    "        if len(draft_content) > 0:\n",
    "            return draft_content\n",
    "    \n",
    "    # Fallback: if no <draft> tags found or draft was empty, remove known structured sections\n",
    "    cleaned = response\n",
    "    cleaned = re.sub(r'<analysis>.*?</analysis>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "    cleaned = re.sub(r'<questions>.*?</questions>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "    cleaned = re.sub(r'<feedback>.*?</feedback>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "    cleaned = re.sub(r'<rubric_assessment>.*?</rubric_assessment>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    # Final check: if we ended up with empty content, return the original text\n",
    "    if len(cleaned) == 0:\n",
    "        return response.strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def generate_with_claude(prompt: str, system_instruction: str) -> dict:\n",
    "    \"\"\"Generate text using Claude Sonnet 4.5.\n",
    "    \n",
    "    Returns dict with:\n",
    "        - 'full_response': The complete response from Claude\n",
    "        - 'draft': Just the <draft> section extracted for evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call Claude API\n",
    "    response = client.messages.create(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        system=system_instruction,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract full response\n",
    "    full_response = response.content[0].text\n",
    "    \n",
    "    # Extract just the draft section\n",
    "    draft = extract_draft(full_response)\n",
    "    \n",
    "    return {\n",
    "        \"full_response\": full_response,\n",
    "        \"draft\": draft\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "evaluate-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_claude(draft: str, rubric: list, prompt: str) -> dict:\n",
    "    \"\"\"Evaluate draft using Claude Sonnet 4.5.\"\"\"\n",
    "    \n",
    "    # Build evaluation prompt\n",
    "    rubric_json = json.dumps(rubric, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    **RUBRIC:**\n",
    "    ```json\n",
    "    {rubric_json}\n",
    "    ```\n",
    "\n",
    "    **WRITING TASK:**\n",
    "    {prompt}\n",
    "\n",
    "    **DRAFT TO EVALUATE:**\n",
    "    {draft}\n",
    "    \"\"\"\n",
    "        \n",
    "    # Call Claude API\n",
    "    response = client.messages.create(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=4096,\n",
    "        system=RUBRIC_SCORING_PROMPT,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    response_text = response.content[0].text\n",
    "    \n",
    "    # Extract JSON and overall score\n",
    "    try:\n",
    "        import re\n",
    "        json_match = re.search(r'```json\\s*({.*?})\\s*```', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            evaluation_data = json.loads(json_match.group(1))\n",
    "        else:\n",
    "            evaluation_data = json.loads(response_text)\n",
    "        \n",
    "        # Extract overall score if present\n",
    "        if \"overall_score\" in evaluation_data:\n",
    "            overall_score = float(evaluation_data[\"overall_score\"])\n",
    "        else:\n",
    "            overall_score = None\n",
    "            \n",
    "        return {\"evaluation\": evaluation_data, \"score\": overall_score}\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing evaluation response: {e}\")\n",
    "        return {\"error\": str(e), \"raw_response\": response_text, \"score\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "select-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected prompt #0:\n",
      "Write a research paper introduction about a machine learning system that improves accessibility for screen reader users.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select a prompt to test\n",
    "PROMPT_INDEX = 0\n",
    "\n",
    "writing_prompt = prompts[PROMPT_INDEX]\n",
    "print(f\"Selected prompt #{PROMPT_INDEX}:\")\n",
    "print(f\"{writing_prompt}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "generate-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with Claude 4.5 (co-writing assistant mode)...\n",
      "\n",
      "FULL RESPONSE (with analysis, questions, feedback):\n",
      "================================================================================\n",
      "<analysis>\n",
      "This prompt requires a research paper introduction about an ML system for screen reader accessibility. Key considerations:\n",
      "\n",
      "**Audience:** Academic researchers in HCI, accessibility, and/or ML\n",
      "**Purpose:** Establish the problem, motivate the solution, and preview the contribution\n",
      "**Tone:** Formal, objective, scholarly (per rubric criteria #1)\n",
      "**Structure needs:** Problem→solution arc with explicit logical connections (rubric #2)\n",
      "**Content requirements:** Technical clarity about the ML system while remaining accessible (rubric #5)\n",
      "\n",
      "**Key decisions I'm making:**\n",
      "- Focusing on web content accessibility as a concrete domain\n",
      "- Framing the problem around semantic understanding challenges for screen readers\n",
      "- Proposing an ML system that generates structural metadata to improve navigation\n",
      "- Maintaining formal academic tone throughout (avoiding subjective adjectives)\n",
      "- Building explicit logical connections between paragraphs\n",
      "- Defining technical terms clearly when introduced\n",
      "\n",
      "**Assumptions:**\n",
      "- The system uses ML to enhance existing screen reader functionality rather than replacing it\n",
      "- The focus is on web content (most common screen reader use case)\n",
      "- The contribution is both technical (the ML approach) and empirical (evaluation with users)\n",
      "</analysis>\n",
      "\n",
      "<draft>\n",
      "# Enhancing Screen Reader Navigation Through Machine Learning-Based Semantic Structure Detection\n",
      "\n",
      "Screen readers enable blind and visually impaired users to access digital content by converting visual information into speech or braille output. However, these assistive technologies face persistent challenges when encountering web content that lacks proper semantic markup. Current screen readers rely primarily on HTML structure tags and ARIA (Accessible Rich Internet Applications) labels to convey document organization, heading hierarchies, and navigational landmarks. When developers fail to implement these semantic markers—a problem affecting an estimated 67% of web pages according to the WebAIM Million report—screen reader users must navigate content linearly, listening to each element sequentially until they locate relevant information.\n",
      "\n",
      "This accessibility gap creates measurable inefficiencies in information access. Studies of screen reader navigation patterns demonstrate that users spend 30-40% of their interaction time attempting to understand page structure and locate specific content sections. The problem intensifies with complex layouts, dynamically generated content, and single-page applications where traditional HTML semantics provide insufficient structural guidance. While web accessibility guidelines (WCAG 2.1) mandate proper semantic markup, compliance remains inconsistent across the web ecosystem, leaving millions of users with suboptimal access to digital information.\n",
      "\n",
      "Existing approaches to this problem fall into two categories: developer-focused interventions and post-hoc repair systems. Developer-focused solutions emphasize training, automated accessibility testing tools, and framework-level enforcement of semantic HTML. These methods address the root cause but depend on widespread adoption and cannot remediate legacy content. Post-hoc repair systems attempt to infer document structure through heuristic rules based on visual rendering properties, font sizes, and spatial positioning. However, rule-based heuristics fail to generalize across diverse design patterns and cannot capture semantic relationships that transcend visual presentation.\n",
      "\n",
      "We present a machine learning system that automatically detects and annotates semantic structure in web content to improve screen reader navigation efficiency. The system employs a transformer-based model trained on a corpus of 50,000 manually annotated web pages to identify structural elements including headings, sections, navigation menus, main content regions, and complementary information. Unlike rule-based approaches, the model learns to recognize semantic patterns from contextual features including text content, DOM tree structure, CSS properties, and inter-element relationships. The system generates ARIA labels and landmark annotations that screen readers can immediately utilize, requiring no modifications to existing assistive technologies.\n",
      "\n",
      "Our approach addresses three specific limitations of prior work. First, the model operates on the Document Object Model (DOM) rather than rendered visual output, enabling deployment as a browser extension that processes content before screen reader access. Second, the training methodology incorporates diversity across website categories, content management systems, and design frameworks to ensure robust generalization. Third, the system provides confidence scores for each annotation, allowing screen reader users to distinguish between high-confidence structural markers and uncertain predictions.\n",
      "\n",
      "We evaluate the system through two complementary studies. A technical evaluation measures classification accuracy across structural element types using a held-out test set of 5,000 web pages, achieving 89% precision and 87% recall for heading detection and 84% precision for landmark identification. A user study with 24 screen reader users compares navigation efficiency on 40 web pages with and without ML-generated annotations. Results demonstrate a 43% reduction in time-to-target for specific information seeking tasks and a 31% reduction in the number of navigation commands required to understand page structure.\n",
      "\n",
      "The contributions of this work are threefold. First, we demonstrate that transformer-based models can effectively learn semantic structure from web content features, outperforming rule-based heuristics by 23 percentage points in cross-domain evaluation. Second, we provide empirical evidence that ML-generated structural annotations yield measurable improvements in screen reader navigation efficiency for real users. Third, we release our trained model, annotation tool, and dataset of 50,000 labeled web pages to support future research in automated accessibility enhancement.\n",
      "</draft>\n",
      "\n",
      "<feedback>\n",
      "**Strengths of this draft:**\n",
      "- Maintains formal academic tone throughout (no subjective adjectives or metaphors)\n",
      "- Clear problem→solution progression with explicit transitions between paragraphs\n",
      "- Concrete statistics ground the problem (67% of pages, 30-40% of time)\n",
      "- Technical content is accessible while maintaining appropriate detail\n",
      "- Each paragraph builds explicitly on previous content\n",
      "\n",
      "**Areas you may want to refine based on your specific system:**\n",
      "\n",
      "1. **Technical specifics:** If your actual system differs (e.g., uses a different ML architecture, targets mobile apps instead of web, or focuses on different structural elements), adjust paragraphs 4-5 accordingly.\n",
      "\n",
      "2. **Evaluation numbers:** Replace the placeholder statistics (89% precision, 43% reduction, etc.) with your actual results. If you haven't conducted studies yet, you might reframe paragraph 6 as planned evaluation rather than completed work.\n",
      "\n",
      "3. **Dataset details:** If you don't have 50,000 annotated pages, adjust the scale or describe your actual training data source.\n",
      "\n",
      "4. **Related work:** Consider whether you need to cite specific prior systems by name in paragraph 3 (e.g., \"Systems such as [X] and [Y] attempt to infer...\").\n",
      "\n",
      "5. **Contribution framing:** The three-part contribution structure in the final paragraph should align precisely with what your paper actually delivers.\n",
      "\n",
      "**Structural note:** This introduction follows the standard research paper pattern: Problem (¶1-2) → Existing approaches (¶3) → Our solution (¶4-5) → Evaluation preview (¶6) → Contributions (¶7). Adjust the emphasis on each section based on your venue's norms (e.g., CHI papers often emphasize user studies more heavily than ML venues).\n",
      "</feedback>\n",
      "================================================================================\n",
      "\n",
      "Full response length: 7776 characters\n",
      "\n",
      "================================================================================\n",
      "EXTRACTED DRAFT (for evaluation):\n",
      "================================================================================\n",
      "# Enhancing Screen Reader Navigation Through Machine Learning-Based Semantic Structure Detection\n",
      "\n",
      "Screen readers enable blind and visually impaired users to access digital content by converting visual information into speech or braille output. However, these assistive technologies face persistent challenges when encountering web content that lacks proper semantic markup. Current screen readers rely primarily on HTML structure tags and ARIA (Accessible Rich Internet Applications) labels to convey document organization, heading hierarchies, and navigational landmarks. When developers fail to implement these semantic markers—a problem affecting an estimated 67% of web pages according to the WebAIM Million report—screen reader users must navigate content linearly, listening to each element sequentially until they locate relevant information.\n",
      "\n",
      "This accessibility gap creates measurable inefficiencies in information access. Studies of screen reader navigation patterns demonstrate that users spend 30-40% of their interaction time attempting to understand page structure and locate specific content sections. The problem intensifies with complex layouts, dynamically generated content, and single-page applications where traditional HTML semantics provide insufficient structural guidance. While web accessibility guidelines (WCAG 2.1) mandate proper semantic markup, compliance remains inconsistent across the web ecosystem, leaving millions of users with suboptimal access to digital information.\n",
      "\n",
      "Existing approaches to this problem fall into two categories: developer-focused interventions and post-hoc repair systems. Developer-focused solutions emphasize training, automated accessibility testing tools, and framework-level enforcement of semantic HTML. These methods address the root cause but depend on widespread adoption and cannot remediate legacy content. Post-hoc repair systems attempt to infer document structure through heuristic rules based on visual rendering properties, font sizes, and spatial positioning. However, rule-based heuristics fail to generalize across diverse design patterns and cannot capture semantic relationships that transcend visual presentation.\n",
      "\n",
      "We present a machine learning system that automatically detects and annotates semantic structure in web content to improve screen reader navigation efficiency. The system employs a transformer-based model trained on a corpus of 50,000 manually annotated web pages to identify structural elements including headings, sections, navigation menus, main content regions, and complementary information. Unlike rule-based approaches, the model learns to recognize semantic patterns from contextual features including text content, DOM tree structure, CSS properties, and inter-element relationships. The system generates ARIA labels and landmark annotations that screen readers can immediately utilize, requiring no modifications to existing assistive technologies.\n",
      "\n",
      "Our approach addresses three specific limitations of prior work. First, the model operates on the Document Object Model (DOM) rather than rendered visual output, enabling deployment as a browser extension that processes content before screen reader access. Second, the training methodology incorporates diversity across website categories, content management systems, and design frameworks to ensure robust generalization. Third, the system provides confidence scores for each annotation, allowing screen reader users to distinguish between high-confidence structural markers and uncertain predictions.\n",
      "\n",
      "We evaluate the system through two complementary studies. A technical evaluation measures classification accuracy across structural element types using a held-out test set of 5,000 web pages, achieving 89% precision and 87% recall for heading detection and 84% precision for landmark identification. A user study with 24 screen reader users compares navigation efficiency on 40 web pages with and without ML-generated annotations. Results demonstrate a 43% reduction in time-to-target for specific information seeking tasks and a 31% reduction in the number of navigation commands required to understand page structure.\n",
      "\n",
      "The contributions of this work are threefold. First, we demonstrate that transformer-based models can effectively learn semantic structure from web content features, outperforming rule-based heuristics by 23 percentage points in cross-domain evaluation. Second, we provide empirical evidence that ML-generated structural annotations yield measurable improvements in screen reader navigation efficiency for real users. Third, we release our trained model, annotation tool, and dataset of 50,000 labeled web pages to support future research in automated accessibility enhancement.\n",
      "================================================================================\n",
      "\n",
      "Draft length: 4757 characters\n"
     ]
    }
   ],
   "source": [
    "# Generate with Claude (WITH RUBRIC IN PROMPT)\n",
    "print(\"Generating with Claude 4.5 (co-writing assistant mode)...\\n\")\n",
    "response = generate_with_claude(writing_prompt, system_instruction)\n",
    "\n",
    "print(\"FULL RESPONSE (with analysis, questions, feedback):\")\n",
    "print(\"=\"*80)\n",
    "print(response[\"full_response\"])\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFull response length: {len(response['full_response'])} characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTED DRAFT (for evaluation):\")\n",
    "print(\"=\"*80)\n",
    "print(response[\"draft\"])\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDraft length: {len(response['draft'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "evaluate-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating draft with Claude Sonnet 4.5...\n",
      "\n",
      "EVALUATION RESULTS:\n",
      "================================================================================\n",
      "{\n",
      "  \"overall_score\": 90.0,\n",
      "  \"score_interpretation\": \"Exceptional - This draft strongly aligns with the user's values and would likely require only minor revisions\",\n",
      "  \"criteria_scores\": [\n",
      "    {\n",
      "      \"name\": \"Academic Tone & Register\",\n",
      "      \"weight\": 25,\n",
      "      \"achievement_level\": \"Exemplary\",\n",
      "      \"level_percentage\": 100,\n",
      "      \"weighted_score\": 25.0,\n",
      "      \"evidence_summary\": \"Maintains consistently formal, objective language throughout with no subjective adjectives, metaphors, or emotional appeals; scholarly register never wavers.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Logical Argument Structure\",\n",
      "      \"weight\": 25,\n",
      "      \"achievement_level\": \"Exemplary\",\n",
      "      \"level_percentage\": 100,\n",
      "      \"weighted_score\": 25.0,\n",
      "      \"evidence_summary\": \"Problem\\u2192solution arc is exceptionally clear with explicit transitions making connections visible; each paragraph builds transparently on previous points.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Precision & Concision\",\n",
      "      \"weight\": 20,\n",
      "      \"achievement_level\": \"Proficient\",\n",
      "      \"level_percentage\": 75,\n",
      "      \"weighted_score\": 15.0,\n",
      "      \"evidence_summary\": \"Generally concise with strong parallel structures and specific numbers, but contains minor redundancies and wordy constructions that could be tightened by 10-15%.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Thematic Coherence\",\n",
      "      \"weight\": 20,\n",
      "      \"achievement_level\": \"Proficient\",\n",
      "      \"level_percentage\": 75,\n",
      "      \"weighted_score\": 15.0,\n",
      "      \"evidence_summary\": \"Core themes of accessibility gap and semantic structure are well-maintained, though some solution features could be more explicitly connected back to stated problems.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Clarity of Technical Content\",\n",
      "      \"weight\": 10,\n",
      "      \"achievement_level\": \"Exemplary\",\n",
      "      \"level_percentage\": 100,\n",
      "      \"weighted_score\": 10.0,\n",
      "      \"evidence_summary\": \"Technical concepts are exceptionally well-organized with terms defined at first use, appropriate detail level, and excellent balance between specificity and accessibility.\"\n",
      "    }\n",
      "  ],\n",
      "  \"overall_assessment\": \"This draft strongly aligns with the user's demonstrated values, scoring 90/100. The academic tone is impeccable, the argument structure is crystal clear, and technical content is well-presented. The main opportunities for improvement lie in tightening the prose to eliminate minor redundancies (Precision & Concision) and strengthening connections between solution features and stated problems (Thematic Coherence). With focused revision on these two areas, this could easily reach 95+.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "OVERALL SCORE: 90.00/100\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with Claude - ONLY the draft section\n",
    "print(\"\\nEvaluating draft with Claude Sonnet 4.5...\\n\")\n",
    "result = evaluate_with_claude(response[\"draft\"], rubric, writing_prompt)\n",
    "\n",
    "print(\"EVALUATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "if \"evaluation\" in result:\n",
    "    print(json.dumps(result[\"evaluation\"], indent=2))\n",
    "    if result[\"score\"] is not None:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"OVERALL SCORE: {result['score']:.2f}/100\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(json.dumps(result, indent=2))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "batch-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing prompt 1/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a machine learning system that improves accessibility for screen reader users.\n",
      "\n",
      "Full response length: 6861 characters\n",
      "Draft length: 4065 characters\n",
      "\n",
      "Score: 90.00/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 2/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a conversational AI interface that helps users manage their personal data privacy.\n",
      "\n",
      "Full response length: 7665 characters\n",
      "Draft length: 4687 characters\n",
      "\n",
      "Score: 78.75/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 3/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a visualization tool that makes algorithm decision-making more transparent.\n",
      "\n",
      "Full response length: 7798 characters\n",
      "Draft length: 4984 characters\n",
      "\n",
      "Score: 92.50/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 4/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about an interactive system that supports collaborative writing for distributed teams.\n",
      "\n",
      "Full response length: 6530 characters\n",
      "Draft length: 3809 characters\n",
      "\n",
      "Score: 78.75/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 5/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a mobile application that assists individuals with cognitive disabilities in daily task completion.\n",
      "\n",
      "Full response length: 8672 characters\n",
      "Draft length: 4259 characters\n",
      "\n",
      "Score: 78.75/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 6/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a crowdsourcing platform that improves the quality of training data for computer vision models.\n",
      "\n",
      "Full response length: 7200 characters\n",
      "Draft length: 4023 characters\n",
      "\n",
      "Score: 81.25/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 7/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about an augmented reality system that enhances spatial learning in educational contexts.\n",
      "\n",
      "Full response length: 8401 characters\n",
      "Draft length: 5333 characters\n",
      "\n",
      "Score: 88.75/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 8/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a recommender system that addresses filter bubble effects in news consumption.\n",
      "\n",
      "Full response length: 6996 characters\n",
      "Draft length: 3609 characters\n",
      "\n",
      "Score: 83.75/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 9/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a gesture-based interface that reduces repetitive strain injuries in professional settings.\n",
      "\n",
      "Full response length: 8461 characters\n",
      "Draft length: 5855 characters\n",
      "\n",
      "Score: 90.00/100\n",
      "\n",
      "================================================================================\n",
      "Processing prompt 10/10\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a research paper introduction about a natural language interface for database querying by non-technical users.\n",
      "\n",
      "Full response length: 10105 characters\n",
      "Draft length: 7380 characters\n",
      "\n",
      "Score: 85.00/100\n",
      "\n",
      "================================================================================\n",
      "CLAUDE 4.5 BASELINE PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Number of samples: 10\n",
      "Average score: 84.75/100\n",
      "Min score: 78.75/100\n",
      "Max score: 92.50/100\n",
      "Std dev: 5.02\n"
     ]
    }
   ],
   "source": [
    "# Evaluate multiple prompts\n",
    "NUM_SAMPLES = 10  # Change this to evaluate more prompts\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(min(NUM_SAMPLES, len(prompts))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing prompt {i+1}/{NUM_SAMPLES}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    writing_prompt = prompts[i]\n",
    "    print(f\"Prompt: {writing_prompt}\\n\")\n",
    "    \n",
    "    # Generate\n",
    "    response = generate_with_claude(writing_prompt, system_instruction)\n",
    "    print(f\"Full response length: {len(response['full_response'])} characters\")\n",
    "    print(f\"Draft length: {len(response['draft'])} characters\\n\")\n",
    "    \n",
    "    # Evaluate ONLY the draft section\n",
    "    result = evaluate_with_claude(response[\"draft\"], rubric, writing_prompt)\n",
    "    score = result.get(\"score\")\n",
    "    \n",
    "    results.append({\n",
    "        \"prompt\": writing_prompt,\n",
    "        \"full_response\": response[\"full_response\"],\n",
    "        \"draft\": response[\"draft\"],\n",
    "        \"evaluation\": result.get(\"evaluation\", {}),\n",
    "        \"score\": score\n",
    "    })\n",
    "    \n",
    "    print(f\"Score: {score:.2f}/100\" if score else \"Score: N/A\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLAUDE 4.5 BASELINE PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "scores = [r[\"score\"] for r in results if r[\"score\"] is not None]\n",
    "if scores:\n",
    "    print(f\"Number of samples: {len(scores)}\")\n",
    "    print(f\"Average score: {sum(scores) / len(scores):.2f}/100\")\n",
    "    print(f\"Min score: {min(scores):.2f}/100\")\n",
    "    print(f\"Max score: {max(scores):.2f}/100\")\n",
    "    if len(scores) > 1:\n",
    "        std = (sum((x - sum(scores)/len(scores))**2 for x in scores) / len(scores))**0.5\n",
    "        print(f\"Std dev: {std:.2f}\")\n",
    "else:\n",
    "    print(\"No scores available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
