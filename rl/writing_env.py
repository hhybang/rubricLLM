"""
RL Environment for Rubric-Based Writing Improvement

This environment trains a language model to iteratively improve writing drafts
based on rubric scores. The model learns to revise writing by receiving rewards
proportional to score improvements.

Episode Structure:
1. User provides a writing prompt
2. Model generates initial draft
3. Evaluator (Claude 4.5) scores draft using rubric -> score_t
4. Model generates revision
5. Evaluator scores revision -> score_t+1
6. Reward = 10 Ã— (score_t+1 - score_t)
7. Repeat until reward <= 0 or max_turns reached
"""

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from typing import Any, Sequence

import anthropic
import chz
import tinker
from tinker_cookbook import renderers
from tinker_cookbook.rl.types import (
    Env,
    EnvGroupBuilder,
    Observation,
    RLDataset,
    RLDatasetBuilder,
    StepResult,
    StopCondition,
)

# Import prompts from the existing prompts module
from rl.prompts import RUBRIC_SCORING_PROMPT, build_writing_assistant_prompt

@dataclass
class WritingTask:
    """Represents a writing task with its prompt and rubric."""

    prompt: str
    rubric: list[dict[str, Any]]
    max_turns: int = 5
    task_id: str = ""


class WritingRevisionEnv(Env):
    """
    Single-episode environment for iterative writing revision.

    The agent receives a writing task and rubric, then iteratively improves
    its writing by receiving rewards based on rubric score changes.
    """

    def __init__(
        self,
        task: WritingTask,
        renderer: renderers.Renderer,
        evaluator_client: anthropic.Anthropic,
        evaluator_model: str = "claude-sonnet-4-5",
    ):
        self.task = task
        self.renderer = renderer
        self.evaluator_client = evaluator_client
        self.evaluator_model = evaluator_model

        # Episode state
        self.current_turn = 0
        self.conversation_history: list[renderers.Message] = []
        self.current_draft = ""
        self.current_score = 0.0
        self.score_history: list[float] = []

        # Use simple writing assistant prompt (no rubric in system instruction)
        self.system_instruction = build_writing_assistant_prompt(None)

    @property
    def stop_condition(self) -> StopCondition:
        return self.renderer.get_stop_sequences()

    async def initial_observation(self) -> tuple[Observation, StopCondition]:
        """
        Returns the initial observation: system instruction + user's writing prompt.
        """
        # Create initial conversation with system instruction and user prompt
        self.conversation_history = [
            {"role": "system", "content": self.system_instruction},
            {"role": "user", "content": self.task.prompt}
        ]

        # Use renderer to build the observation (like ProblemEnv does)
        observation = self.renderer.build_generation_prompt(self.conversation_history)

        return observation, self.stop_condition

    async def step(self, action: list[int]) -> StepResult:
        """
        Process the model's action (generated text) and compute reward.

        Args:
            action: Token IDs generated by the model

        Returns:
            StepResult with reward, next observation, and episode termination flag
        """
        # Parse the model's response using the renderer
        message, parse_success = self.renderer.parse_response(action)
        generated_text = message["content"]

        # Update conversation history
        self.conversation_history.append(message)

        # Extract the draft from the generated text
        self.current_draft = self._extract_draft(generated_text)

        # Score the current draft using the evaluator
        new_score = await self._score_draft(self.current_draft)

        # Compute reward based on score improvement
        if self.current_turn == 0:
            # First turn: reward is just the score (no previous score to compare)
            reward = new_score / 10.0  # Normalize to similar scale
            score_delta = 0.0
        else:
            # Subsequent turns: reward proportional to score improvement
            score_delta = new_score - self.current_score
            reward = 10.0 * (score_delta / 100.0)  # Scale to reasonable range

        # Update state
        self.score_history.append(new_score)
        prev_score = self.current_score
        self.current_score = new_score
        self.current_turn += 1

        # Determine if episode should end
        # Only stop if max turns reached or perfect score - don't stop on negative reward
        episode_done = (
            self.current_turn >= self.task.max_turns  # Max turns reached
            or new_score >= 99.5  # Near-perfect score achieved
        )

        # Prepare next observation if episode not done
        if not episode_done:
            # Ask the model to revise based on the score
            revision_prompt = self._create_revision_prompt(new_score, prev_score)
            self.conversation_history.append({
                "role": "user",
                "content": revision_prompt
            })

            # Use renderer to build next observation
            next_observation = self.renderer.build_generation_prompt(self.conversation_history)
            next_stop_condition = self.stop_condition
        else:
            # Episode done - return empty observation
            next_observation = tinker.ModelInput.empty()
            next_stop_condition = self.stop_condition

        # Collect metrics
        metrics = {
            "turn": self.current_turn,
            "score": new_score,
            "score_delta": score_delta,
            "avg_score": sum(self.score_history) / len(self.score_history),
            "max_score": max(self.score_history),
        }

        return StepResult(
            reward=reward,
            episode_done=episode_done,
            next_observation=next_observation,
            next_stop_condition=next_stop_condition,
            metrics=metrics,
        )

    def _extract_draft(self, generated_text: str) -> str:
        """
        Extract the actual writing draft from the model's response.
        The model may include <analysis>, <questions>, <draft>, <feedback> sections.
        We only want the <draft> section for evaluation.
        """
        # Try to extract content between <draft> tags
        draft_match = re.search(r'<draft>(.*?)</draft>', generated_text, re.DOTALL | re.IGNORECASE)
        if draft_match:
            draft_content = draft_match.group(1).strip()
            # Verify we actually got meaningful content (not just whitespace or empty)
            if len(draft_content) > 0:
                return draft_content

        # Fallback: if no <draft> tags found or draft was empty, remove known structured sections
        # and return remaining content
        cleaned = generated_text

        # Remove structured sections
        cleaned = re.sub(r'<analysis>.*?</analysis>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        cleaned = re.sub(r'<questions>.*?</questions>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        cleaned = re.sub(r'<feedback>.*?</feedback>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        cleaned = re.sub(r'<rubric_assessment>.*?</rubric_assessment>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)

        # Clean up whitespace
        cleaned = cleaned.strip()

        # Final check: if we ended up with empty content, return the original text
        # This handles edge cases where the model doesn't use the structured format
        if len(cleaned) == 0:
            return generated_text.strip()

        return cleaned

    def _create_revision_prompt(self, current_score: float, previous_score: float) -> str:
        """Create a prompt asking the model to revise based on the score."""
        if current_score > previous_score:
            feedback = f"Good improvement! Your draft scored {current_score:.1f}/100 (up from {previous_score:.1f}). Let's make it even better."
        elif current_score < previous_score:
            feedback = f"The revision scored {current_score:.1f}/100 (down from {previous_score:.1f}). Let's try a different approach."
        else:
            feedback = f"The draft scored {current_score:.1f}/100 (no change). Let's try to improve it further."

        return f"{feedback}\n\nPlease revise your draft to better meet the rubric criteria. Provide your improved version."

    async def _score_draft(self, draft: str) -> float:
        """
        Score the draft using Claude 4.5 as an evaluator with the rubric.

        Returns:
            Score between 0-100
        """
        import asyncio

        # Add a small delay to avoid hitting rate limits
        # This helps prevent API retry spam when running multiple environments in parallel
        await asyncio.sleep(0.5)

        # Format the rubric for the evaluator
        rubric_text = json.dumps(self.task.rubric, indent=2)

        # Create evaluation prompt
        eval_prompt = f"""
            Please evaluate the following draft against the provided rubric.

            ## Draft to Evaluate:
            {draft}

            ## Rubric:
            {rubric_text}

            {RUBRIC_SCORING_PROMPT}
            """

        try:
            # Call Claude 4.5 to score the draft
            response = self.evaluator_client.messages.create(
                model=self.evaluator_model,
                max_tokens=4096,
                messages=[
                    {"role": "user", "content": eval_prompt}
                ],
            )

            # Extract score from response
            content = response.content[0].text
            score = self._extract_score_from_evaluation(content)
            return score

        except Exception as e:
            print(f"Error scoring draft: {e}")
            # Return neutral score on error
            return 50.0

    def _extract_score_from_evaluation(self, evaluation_text: str) -> float:
        """
        Extract the overall score from Claude's evaluation response.
        Looks for JSON with "overall_score" field.
        """
        try:
            # Find JSON block in the response
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', evaluation_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                data = json.loads(json_str)
                return float(data.get("overall_score", 50.0))

            # Fallback: look for "overall_score" directly
            score_match = re.search(r'"overall_score"\s*:\s*([0-9.]+)', evaluation_text)
            if score_match:
                return float(score_match.group(1))

            # If no score found, return neutral
            return 50.0

        except Exception as e:
            print(f"Error extracting score: {e}")
            return 50.0


class WritingEnvGroupBuilder(EnvGroupBuilder):
    """
    Builds a group of writing revision environments.
    Each environment in the group uses the same task but produces independent trajectories.
    """

    def __init__(
        self,
        task: WritingTask,
        group_size: int,
        renderer: renderers.Renderer,
        evaluator_client: anthropic.Anthropic,
        evaluator_model: str = "claude-sonnet-4-5",
    ):
        self.task = task
        self.group_size = group_size
        self.renderer = renderer
        self.evaluator_client = evaluator_client
        self.evaluator_model = evaluator_model

    async def make_envs(self) -> Sequence[Env]:
        """Create a group of independent environments for the same task."""
        return [
            WritingRevisionEnv(
                task=self.task,
                renderer=self.renderer,
                evaluator_client=self.evaluator_client,
                evaluator_model=self.evaluator_model,
            )
            for _ in range(self.group_size)
        ]

    def logging_tags(self) -> list[str]:
        """Tags for organizing metrics."""
        return [self.task.task_id] if self.task.task_id else []


class WritingRLDataset(RLDataset):
    """
    Dataset of writing tasks for RL training.
    Each batch contains multiple writing tasks.
    """

    def __init__(
        self,
        tasks: list[WritingTask],
        groups_per_batch: int,
        group_size: int,
        renderer: renderers.Renderer,
        evaluator_client: anthropic.Anthropic,
        evaluator_model: str = "claude-sonnet-4-5",
    ):
        self.tasks = tasks
        self.groups_per_batch = groups_per_batch
        self.group_size = group_size
        self.renderer = renderer
        self.evaluator_client = evaluator_client
        self.evaluator_model = evaluator_model

    def __len__(self) -> int:
        """Number of batches in the dataset."""
        return (len(self.tasks) + self.groups_per_batch - 1) // self.groups_per_batch

    def get_batch(self, index: int) -> Sequence[EnvGroupBuilder]:
        """Get a batch of environment group builders."""
        start = index * self.groups_per_batch
        end = min(len(self.tasks), start + self.groups_per_batch)

        builders: list[EnvGroupBuilder] = []
        for i in range(start, end):
            task = self.tasks[i]
            builders.append(
                WritingEnvGroupBuilder(
                    task=task,
                    group_size=self.group_size,
                    renderer=self.renderer,
                    evaluator_client=self.evaluator_client,
                    evaluator_model=self.evaluator_model,
                )
            )

        return builders


@chz.chz
class WritingRLDatasetBuilder(RLDatasetBuilder):
    """
    Builder for creating training and evaluation datasets.
    """

    # Dataset configuration
    prompts: list[str]  # List of writing prompts
    rubric: list[dict[str, Any]]  # The rubric to use for all tasks
    groups_per_batch: int = 8
    group_size: int = 4
    max_turns: int = 5

    # Model configuration (needed for renderer)
    model_name: str  # Policy model name
    renderer_name: str | None = None  # Override renderer if needed

    # Evaluator configuration
    evaluator_api_key: str | None = None
    evaluator_model: str = "claude-sonnet-4-5"

    async def __call__(self) -> tuple[RLDataset, RLDataset | None]:
        """Build training and optional evaluation datasets."""
        import os
        from tinker_cookbook import model_info
        from tinker_cookbook.tokenizer_utils import get_tokenizer

        # Initialize evaluator client
        api_key = self.evaluator_api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY must be set or provided")

        evaluator_client = anthropic.Anthropic(api_key=api_key)

        # Create renderer for the policy model
        tokenizer = get_tokenizer(self.model_name)
        if self.renderer_name:
            renderer = renderers.get_renderer(self.renderer_name, tokenizer)
        else:
            renderer_name = model_info.get_recommended_renderer_name(self.model_name)
            renderer = renderers.get_renderer(renderer_name, tokenizer)

        # Create tasks from prompts
        tasks = [
            WritingTask(
                prompt=prompt,
                rubric=self.rubric,
                max_turns=self.max_turns,
                task_id=f"task_{i}",
            )
            for i, prompt in enumerate(self.prompts)
        ]

        # Create training dataset
        train_dataset = WritingRLDataset(
            tasks=tasks,
            groups_per_batch=self.groups_per_batch,
            group_size=self.group_size,
            renderer=renderer,
            evaluator_client=evaluator_client,
            evaluator_model=self.evaluator_model,
        )

        # No separate eval dataset for now
        return train_dataset, None
